# rcnn

0, 常用概念

1, 原理

2, 意义

3, 训练方法

# 常用概念

1, Bounding Box(bbox)

bbox是包含物体的最小矩形，该物体应在最小矩形内部, 
物体检测中关于物体位置的信息输出是一组(x,y,w,h)数据，
其中x,y代表着bbox的左上角(或者其他固定点，可自定义)，对应的w,h表示bbox的宽和高.一组(x,y,w,h)可以唯一的确定一个定位框。

2, Intersection over Union(IoU)

对于两个区域R和R′,则两个区域的重叠程度overlap计算如下:

O(R,R′)=|R∩R′|/|R∪R′|

如下图：

![image](https://user-images.githubusercontent.com/37278270/131203527-da74d627-d0ef-4e20-9b0d-1bcb5d68e218.png)

3， 非极大值抑制(Non-Maximum Suppression又称NMS)

非极大值抑制，简称为NMS算法，英文为Non-Maximum Suppression。其思想是搜素局部最大值，抑制极大值。NMS算法在不同应用中的具体实现不太一样，但思想是一样的。

使用方法：

前提：目标边界框列表及其对应的置信度得分列表，设定阈值，阈值用来删除重叠较大的边界框。

IoU：intersection-over-union，即两个边界框的交集部分除以它们的并集。

非极大值抑制的流程如下：

根据置信度得分进行排序

选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除

计算所有边界框的面积

计算置信度最高的边界框与其它候选框的IoU。

删除IoU大于阈值的边界框

重复上述过程，直至边界框列表为空


# 原理

如下图：

![image](https://user-images.githubusercontent.com/37278270/131203707-7cd62a03-4cc2-48ec-bd6d-9981f2653f3c.png)


借鉴了滑动窗口思想，R-CNN 采用对区域进行识别的方案。

具体是：

1, 给定一张输入图片，从图片中提取 2000 个类别独立的候选区域。

2, 对于每个区域利用 CNN 抽取一个固定长度的特征向量。

3, 再对每个区域利用 SVM 进行目标分类。

如下图：

![image](https://user-images.githubusercontent.com/37278270/131203795-b9c6a1e0-1035-484a-a6fa-d263f779e4d4.png)

# 意义
1, 在 Pascal VOC 2012 的数据集上，能够将目标检测的验证指标 mAP 提升到 53.3%,这相对于之前最好的结果提升了整整 30%.

2, 这一方法证明了可以将神经网络应用在自底向上的候选区域，这样就可以进行目标分类和目标定位。

3, 这一方法也带来了一个观点，那就是当你缺乏大量的标注数据时，比较好的可行的手段是，进行神经网络的迁移学习，采用在其他大型数据集训练过后的神经网络，
然后在小规模特定的数据集中进行 fine-tune 微调。


# 训练方法
1.预训练

训练的时候，文章用了个trick，他先用ILSVRC2012数据库训练Alexnet，训练的时候目标时图片分类，因为ILSVRC2012数据库没有分类的标定数据。这步称为预训练。

2.fine-tuning
这种方法也是当数据量不够的时候，常用的一种训练方式，即先用别的数据库训练网络，然后再用自己的数据库微调训练(fine-tuning)。

微调期间，定义与ground truth的IoU大于0.5的候选区域为正样本，其余的为负样本。

这里训练时，网络输出要有所改变，因为分类问题，网络输出为N+1，其中N为正样本的类别数，1为背景。
对于VOC，N=20，对于ILSVRC2013, N=200。

3.目标分类
因为最终目标分类是通过SVM进行分类的，而不是通过网络框架中的softmax分类的。

下面先说一下在SVM的训练中，正负样本的定义，为什么这样定义，然后再说一下为什么不直接用softmax输出的结果而是再训练SVM来进行分类的。

1）SVM正负样本的定义，为什么fine-tuning与SVM正负样本定义不一样？
在训练SVM时，正样本为groundtruth，负样本定义为与ground truth的IoU小于0.3的候选区域为负样本，介于0.3与0.7之间的样本忽略。

fine-tuning时担心过拟合的原因，要扩大正样本的样本量，所以定义比较宽松，但是SVM是最终用于分类的分类器，而且SVM原理就是最小的距离最大化，越难分的数据越有利于SVM的训练，所以对样本的定义比较严格。

2）为什么不直接用softmax的输出结果？

因为在训练softmax的时候数据本来就不是很准确，而SVM的训练使用的是hard negative也就是样本比较严格，所以SVM效果会更好。

4.回归器训练

回归器是线性的，输入为Alexnet pool5的输出。

bbox回归认为候选区域和ground-truth之间是线性关系(因为在最后从SVM内确定出来的区域比较接近ground-truth,这里近似认为可以线性关系)

训练回归器的输入为N对值，，分别为候选区域的框坐标和真实的框坐标，下面在不必要时省略i。这里选用的Proposal必须和Ground Truth的IoU＞0.6才算是正样本.

从候选框P到预测框的基本思路如下：

因为我们在分类之后得到候选框P ，其中和为候选框的中心点，和为候选框的宽高，

下面介绍中所有框的定位都用这种定义，即x和y表示中心点坐标，w和h表示框的宽高。知道候选框的表示，那么只要估计出出候选框与真实框的平移量和尺度缩放比例，就可以得到我们的估计框了

1）先求平移量(Δx,Δy)

Δx= , Δy=

即R-CNN论文里面的:
,

2）算尺度放缩量



我们要学习的是这四个变换，就可以得到估计框了。

这四个变换可以用下列公式表示：

上式中为Alexnet pool5输出的特征，所以要求这四个变换，只需求出即可。

该回归器的损失函数为：

上式中的可以通过如下公式求出：




所以通过输入的特征值训练，从而求出，就可以得到回归器





















# 参考
https://blog.csdn.net/briblue/article/details/82012575

https://arxiv.org/abs/1311.2524

https://www.jianshu.com/p/5056e6143ed5

https://www.jianshu.com/p/d452b5615850


